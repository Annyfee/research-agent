import asyncio
import json

import logging  # <--- è®°å¾—å¯¼å…¥ logging

# --- æ¶ˆéŸ³ä»£ç  --- ç­‰çº§ä½äºWarningçš„æç¤ºå…¨éƒ¨å±è”½
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("httpcore").setLevel(logging.WARNING)
logging.getLogger("openai").setLevel(logging.WARNING)
logging.getLogger("chromadb").setLevel(logging.WARNING)
logging.getLogger("mcp").setLevel(logging.WARNING)
# -----------------------


from langchain_core.messages import SystemMessage, ToolMessage, HumanMessage
# [æ–°å¢ 1] å¼•å…¥ tool è£…é¥°å™¨å’Œ RAG ä»“åº“
from langchain_core.tools import tool
from tools.rag_store import RAGStore

from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import MessagesState
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from loguru import logger

from config import OPENAI_API_KEY
from tools.stream import run_agent_with_streaming

# [æ–°å¢ 2] åˆå§‹åŒ– RAG (å•ä¾‹æ¨¡å¼)
# è¿™ä¸€æ­¥ä¼šåŠ è½½ rag_store.py é‡Œçš„é…ç½® (æœ¬åœ°/äº‘ç«¯)
rag = RAGStore()

MCP_SERVERS = {
    "æœç´¢æœåŠ¡": {
        "transport": "streamable_http",
        "url": "http://0.0.0.0:8002/mcp"
    }
}


# [æ–°å¢ 3] å®šä¹‰ RAG æ£€ç´¢å·¥å…· (ç»™ Agent æŸ¥åº“ç”¨)
@tool
def search_knowledge_base(query: str):
    """
    å½“ç³»ç»Ÿæç¤º'èµ„æ–™å·²å­˜å…¥çŸ¥è¯†åº“'æ—¶ï¼Œæˆ–è€…éœ€è¦å›ç­”åŸºäºäº‹å®çš„é—®é¢˜æ—¶ï¼Œ
    å¿…é¡»è°ƒç”¨æ­¤å·¥å…·ä»æœ¬åœ°çŸ¥è¯†åº“(RAG)ä¸­æ£€ç´¢ã€‚
    """
    logger.info(f"ğŸ“š Agent æ­£åœ¨æŸ¥è¯¢çŸ¥è¯†åº“: {query}")
    results = rag.query(query)

    if not results:
        return "çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚"

    # æ ¼å¼åŒ–è¿”å›ç»“æœ
    formatted_res = []
    for doc in results:
        source = doc.metadata.get('source', 'unknown')
        score = doc.metadata.get('rerank_score', 0)
        formatted_res.append(f"[æ¥æº: {source} | ç½®ä¿¡åº¦: {score:.2f}]\n{doc.page_content}")

    return "\n\n---\n\n".join(formatted_res)


# [æ–°å¢ 4] å®šä¹‰å¤„ç†å™¨èŠ‚ç‚¹ (æ ¸å¿ƒæ‹¦æˆªé€»è¾‘)
async def processor_node(state: MessagesState):
    """
    æ‹¦æˆªå™¨ï¼šç›‘å¬ MCP æŠ“å–å·¥å…·ï¼Œè‡ªåŠ¨å­˜å…¥ RAG å¹¶ç¼©å‡ä¸Šä¸‹æ–‡
    """
    messages = state["messages"]
    last_msg = messages[-1]

    # åªå¤„ç† ToolMessage
    if isinstance(last_msg, ToolMessage):
        # æ‹¦æˆªç›®æ ‡ï¼šMCP çš„æŠ“å–å·¥å…·å (éœ€ä¸ mcp_server_search.py ä¸€è‡´)
        if last_msg.name in ["get_page_content", "batch_fetch"]:

            content = last_msg.content
            # ç®€å•æ ¡éªŒ
            if content and len(str(content)) > 50:
                logger.info(f"ğŸ•µï¸ [Processor] æ•è·åˆ°æŠ“å–æ•°æ® (é•¿åº¦: {len(str(content))})")

                # A. å­˜å…¥ RAG
                rag.add_documents(str(content), source_url=f"tool_call_{last_msg.tool_call_id}")

                # B. æ›¿æ¢è®°å¿†
                new_content = (f"âœ… [ç³»ç»Ÿé€šçŸ¥] ...")  # å†…å®¹ä¸å˜

                return {
                    "messages": [
                        ToolMessage(
                            content=new_content,
                            tool_call_id=last_msg.tool_call_id,
                            name=last_msg.name,
                            # ğŸ”¥ğŸ”¥ğŸ”¥ã€æ–°å¢è¿™è¡Œã€‘æ ¸å¿ƒä¿®å¤ï¼ï¼ï¼ğŸ”¥ğŸ”¥ğŸ”¥
                            # åªæœ‰ç»§æ‰¿äº†ä¸Šä¸€æ¡æ¶ˆæ¯çš„ IDï¼ŒLangGraph æ‰ä¼šæ‰§è¡Œâ€œè¦†ç›–â€æ“ä½œï¼Œè€Œä¸æ˜¯â€œè¿½åŠ â€
                            id=last_msg.id
                        )
                    ]
                }
    return {}


def build_graph(available_tools):
    if not available_tools:
        print('âš ï¸ æœªåŠ è½½ä»»ä½•å·¥å…·')

    # [ä¿®æ”¹ A] åˆå¹¶å·¥å…·ï¼šMCPå·¥å…· + RAGæŸ¥è¯¢å·¥å…·
    all_tools = available_tools + [search_knowledge_base]

    llm = ChatOpenAI(
        model="deepseek-chat",
        api_key=OPENAI_API_KEY,
        base_url="https://api.deepseek.com",
        streaming=True
    )

    # [ä¿®æ”¹ B] æ›´æ–° Promptï¼Œæ•™ä¼š Agent å·¥ä½œæµ
    sys_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ç ”ç©¶åŠ©æ‰‹ã€‚å·¥ä½œæµç¨‹ï¼š\n"
        "1. æœç´¢(web_search) -> 2. æŠ“å–(batch_fetch) -> "
        "3. [ç³»ç»Ÿä¼šè‡ªåŠ¨å­˜å…¥RAG] -> 4. ä½ å¿…é¡»è°ƒç”¨ 'search_knowledge_base' é˜…è¯»å†…å®¹ -> 5. å›ç­”ã€‚"
    )

    # ç»‘å®šåˆå¹¶åçš„å·¥å…·åˆ—è¡¨
    llm_with_tools = llm.bind_tools(all_tools)
    tool_node = ToolNode(all_tools)

    # ä½ çš„åŸç‰ˆ agent_node (å®Œå…¨ä¿æŒä¸å˜)
    async def agent_node(state: MessagesState):
        formatted_msg = []
        for msg in state["messages"]:
            if isinstance(msg, ToolMessage) and not isinstance(msg.content, str):
                formatted_msg.append(
                    ToolMessage(
                        content=json.dumps(msg.content, ensure_ascii=False),
                        tool_call_id=msg.tool_call_id
                    )
                )
            else:
                formatted_msg.append(msg)
        message_for_llm = [SystemMessage(content=sys_prompt)] + formatted_msg
        response = await llm_with_tools.ainvoke(message_for_llm)
        return {"messages": [response]}

    def should_continue(state: MessagesState):
        last_msg = state["messages"][-1]
        if hasattr(last_msg, "tool_calls") and last_msg.tool_calls:
            return "tools"
        else:
            return END

    workflow = StateGraph(MessagesState)

    workflow.add_node("agent", agent_node)
    workflow.add_node("tools", tool_node)

    # [æ–°å¢ 5] æ³¨å†Œ processor èŠ‚ç‚¹
    workflow.add_node("processor", processor_node)

    workflow.add_edge(START, "agent")
    workflow.add_conditional_edges(
        "agent",
        should_continue,
        {
            "tools": "tools",
            END: END
        }
    )

    # [ä¿®æ”¹ C] æ”¹å˜æµå‘ï¼šTools -> Processor -> Agent
    workflow.add_edge("tools", "processor")
    workflow.add_edge("processor", "agent")  # ä»¥å‰æ˜¯ tools -> agent

    return workflow.compile(MemorySaver())


async def chat_loop(app):
    thread_id = "user_123"
    config = {"configurable": {"thread_id": thread_id}}
    while 1:
        user_input = input('\n\nğŸ‘¤ ä½ :').strip()
        # å¢åŠ ä¸€ä¸ªé€€å‡ºåˆ¤æ–­ï¼Œæ–¹ä¾¿è°ƒè¯•
        if not user_input or user_input in ["exit", "quit"]:
            break
        await run_agent_with_streaming(app, user_input, config)


async def main():
    print("ğŸ”Œ æ­£åœ¨åˆå§‹åŒ–MCPå®¢æˆ·ç«¯...")

    client = MultiServerMCPClient(MCP_SERVERS)
    tools = await client.get_tools()
    print(f"âœ…ï¸ æˆåŠŸåŠ è½½å·¥å…·{[t.name for t in tools]}")

    app = build_graph(tools)
    await chat_loop(app)


if __name__ == '__main__':
    asyncio.run(main())