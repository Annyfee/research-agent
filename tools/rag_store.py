import os
import shutil

# å¯¼å…¥ Flashrank (Reranker å§‹ç»ˆç”¨è½»é‡çº§æœ¬åœ°ç‰ˆ)
from flashrank import Ranker, RerankRequest
# LangChain ç»„ä»¶
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from loguru import logger

# å¯¼å…¥é…ç½®
from config import USE_LOCAL_EMBEDDING, EMBEDDING_API_KEY, EMBEDDING_BASE_URL, EMBEDDING_MODEL_NAME


class RAGStore:
    def __init__(self):
        logger.info(f"ğŸš€ [Init] åˆå§‹åŒ– RAG ç³»ç»Ÿ | æ¨¡å¼: {'çº¯æœ¬åœ°' if USE_LOCAL_EMBEDDING else 'äº‘ç«¯API'}")

        # Embedding
        if USE_LOCAL_EMBEDDING:
            # ã€æœ¬åœ°æ¨¡å¼ã€‘åŠ è½½ HuggingFace æ¨¡å‹ (åƒå†…å­˜ï¼Œçœé’±)
            from langchain_huggingface import HuggingFaceEmbeddings
            logger.info(f"ğŸ“¥ æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹: {EMBEDDING_MODEL_NAME} (è¯·ç¡®ä¿æ˜¾å­˜/å†…å­˜å……è¶³)...")
            self.embedding = HuggingFaceEmbeddings(
                model_name=EMBEDDING_MODEL_NAME,
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True} # è¾“å‡ºçš„å‘é‡åšå½’ä¸€åŒ–ï¼Œæ–¹ä¾¿åç»­åšç›¸ä¼¼åº¦æœç´¢
            )
        else:
            # ã€äº‘ç«¯æ¨¡å¼ã€‘è°ƒç”¨ SiliconFlow API (çœå†…å­˜ï¼Œæé€Ÿ)
            from langchain_openai import OpenAIEmbeddings
            if not EMBEDDING_API_KEY.startswith("sk-"):
                logger.error("âŒ æœªé…ç½® EMBEDDING_API_KEYï¼Œæ— æ³•ä½¿ç”¨äº‘ç«¯æ¨¡å¼ï¼")
                raise ValueError("API Key Missing")

            logger.info(f"â˜ï¸ æ­£åœ¨è¿æ¥äº‘ç«¯ Embedding: {EMBEDDING_MODEL_NAME}...")
            self.embedding = OpenAIEmbeddings(
                model=EMBEDDING_MODEL_NAME,
                openai_api_key=EMBEDDING_API_KEY,
                openai_api_base=EMBEDDING_BASE_URL,
                check_embedding_ctx_length=False # è·³è¿‡é•¿åº¦æ£€æŸ¥ï¼Œé¿å…æŠ¥é”™
            )

        # Reranker:ç²¾æ’åº (Flashrank:ä¸ºäº†é€‚åº”æ ¼å¼ï¼Œåœ¨ç²¾æ’åºå‰åè¦è½¬æ¢åè®®)
        # Flashrank åªæœ‰ 100MBï¼Œ4G æœåŠ¡å™¨å®Œå…¨è·‘å¾—åŠ¨ï¼Œä¸ºäº†é€»è¾‘ç®€å•ï¼Œä¿æŒæœ¬åœ°è¿è¡Œ
        self.reranker = Ranker(
            model_name="ms-marco-MiniLM-L-12-v2",
            cache_dir="./models"
        )

        # åˆ‡åˆ†å™¨
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=1200,
            chunk_overlap=250,
            # æ˜ç¡®æŒ‡æ˜åˆ‡å‰²æ–¹æ³•ï¼ŒæŒ‰è¿™ä¸ªé¡ºåºä¾æ¬¡å¾€åæ’ï¼ˆå¦‚æœä¸æŒ‡å®šä¼šé»˜è®¤åˆ‡ï¼‰
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", " "]
        )

        # å‘é‡åº“
        self.vector_store = Chroma(
            persist_directory="./chroma_db",
            # é€‰æ‹©ç”¨è¯¥æ¨¡å‹æ¥åšembeddingçš„å·¥ä½œ
            embedding_function=self.embedding
        )
        logger.info("âœ… [Init] RAG ç³»ç»Ÿå°±ç»ª")

    # RAG - ç¦»çº¿æ¨¡å—(åŠ è½½ä¸åˆ‡å—/å‘é‡åŒ–/å­˜å…¥å‘é‡æ•°æ®åº“)
    def add_documents(self, text_content: str, source_url: str = ""):
        """
        å­˜å…¥å‘é‡æ•°æ®åº“ (è‡ªåŠ¨åˆ†æ‰¹å¤„ç†)
        text_content:éœ€è¦å­˜å‚¨çš„åŸå§‹æ–‡æœ¬å†…å®¹
        source_url:æ–‡æœ¬çš„æ¥æºæ ‡è¯†ï¼Œç”¨äºåç»­æ£€ç´¢æ—¶å±•ç¤ºå‡ºå¤„ (æ–¹ä¾¿AIæ ‡è¯†ç²¾ç¡®æ¥æºï¼Œæ¯”å¦‚url)
        """
        if not text_content or len(text_content) < 50:
            logger.warning("âš ï¸ å†…å®¹è¿‡çŸ­ï¼Œè·³è¿‡å…¥åº“")
            return False

        # å°è£… Document(Documentæ˜¯langchainå›ºå®šæ¥æ”¶çš„å¯¹è±¡æ ¼å¼) metadataåˆ™æŒ‡æ˜å…·ä½“èº«ä»½
        raw_doc = Document(page_content=text_content, metadata={"source": source_url})
        # åˆ‡ç‰‡
        chunks = self.splitter.split_documents([raw_doc])

        # --- [ä¿®å¤æ ¸å¿ƒ] åˆ†æ‰¹å…¥åº“ ---
        # ç¡…åŸºæµåŠ¨é™åˆ¶å•æ¬¡ batch <= 64ï¼Œæˆ‘ä»¬è®¾ä¸º 50 æ¯”è¾ƒå®‰å…¨
        batch_size = 50
        total_chunks = len(chunks)

        for i in range(0, total_chunks, batch_size):
            batch = chunks[i: i + batch_size]
            # è°ƒç”¨å‘é‡åº“å†…ç½®æ–¹æ³•:å°†è¿™ä¸€æ‰¹æ¬¡(50ä¸ª)æ–‡æœ¬ç‰‡æ®µå‘é€ç»™ Embeddingæ¨¡å‹è¿›è¡Œå‘é‡åŒ–ï¼Œå†å°†ç”Ÿæˆçš„å‘é‡è¿åŒåŸå§‹æ–‡æœ¬ã€å…ƒæ•°æ®ä¸€åŒæŒä¹…åŒ–å­˜å‚¨åˆ°æœ¬åœ°Chromaæ•°æ®åº“ä¸­
            # ç®€å•è®²ï¼Œæ­¤å¤„å›Šæ‹¬äº† æ–‡æœ¬å‘é‡åŒ–+å­˜å…¥å‘é‡æ•°æ®åº“ ä¸¤æ­¥
            # æ³¨æ„:åœ¨æ­¤æ­¥å‰ï¼Œæˆ‘ä»¬çš„batchä¸€ç›´éƒ½è¿˜æ˜¯éå‘é‡å½¢æ€
            self.vector_store.add_documents(batch)
            logger.info(f"ğŸ’¾ [Store] åˆ†æ‰¹å…¥åº“: {len(batch)} ä¸ªç‰‡æ®µ ({i + len(batch)}/{total_chunks})")

        logger.info(f"âœ… [Store] å…¨éƒ¨å…¥åº“å®Œæˆ (å…± {total_chunks} ä¸ªç‰‡æ®µ | æ¥æº: {source_url})")
        return True

    # RAG - åœ¨çº¿æ¨¡å—(ç²—æ’/ç²¾æ’/è¿‡æ»¤)
    def query(self, question: str, k_retrieve=50, k_final=6, score_threshold=0.7):
        """
        æ£€ç´¢æµç¨‹: å‘é‡ç²—æ’ -> Flashrank ç²¾æ’
        ç²—æ’ - è®¡ç®—æ•°å­¦è·ç¦»ï¼ˆé•¿å¾—åƒå°±è¡Œï¼‰ï¼›
        ç²¾æ’ - è¿›è¡Œè¯­ä¹‰å¯¹é½ï¼ˆä»”ç»†ç†è§£å‡ºæ ¸å¿ƒé€»è¾‘ï¼‰
        """
        # Phase 1: ç²—æ’
        logger.info(f"ğŸ” [Search] å‘é‡æ£€ç´¢ Top-{k_retrieve}...")
        # è¿™ä¸ªdocä¸åé¢çš„Document(xx)æŒ‡å‘åŒä¸€ä¸ªå‚æ•°å°è£…ï¼Œæ˜¯å› ä¸ºäºŒè€…(Chroma/langchain-langchain_chroma)å·²ç»äº’ç›¸é›†æˆå¥½
        docs = self.vector_store.similarity_search(question, k=k_retrieve)

        if not docs:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
            return []

        # Phase 2: ç²¾æ’
        logger.info(f"âš¡ï¸ [Rerank] Flashrank é‡æ’åº...")
        # æŠŠæ•°æ®å°è£…æˆFlashRankæ¥å—çš„æ ¼å¼(doc.page_contentæ˜¯åŸå§‹æ–‡æœ¬å†…å®¹;doc.metadataæ˜¯æ¡£æ¡ˆæ¥æºï¼šæ¯”å¦‚ä½ å¯ä»¥æŒ‡å®šä¸ºlast_msg.tool_call_id)
        # FlashRankæ˜¯é’ˆå¯¹ç²¾æ’åºçš„ã€‚æ‰€ä»¥è¿™é‡Œåœ¨æ•°æ®ä¼ è¿‡å»ä¸ä¼ å›æ¥éƒ½éœ€è¦è°ƒæ•´æ ¼å¼ã€‚
        passages = []
        for i,doc in enumerate(docs):
            passages.append({"id": str(i), "text": doc.page_content, "meta": doc.metadata})

        # for i, doc in enumerate(docs):
        #     print(doc,'\n')
        #     print(doc.page_content,'\n')
        #     print(doc.metadata,'\n')

        rerank_request = RerankRequest(query=question, passages=passages)
        results = self.reranker.rerank(rerank_request)
        # print(results)

        # Phase 3: è¿‡æ»¤
        final_docs = []
        # å¿…é¡»å¾—åˆ†è¶…è¿‡0.6æ‰èƒ½è¿”å›
        for res in results:
            if res['score'] >= score_threshold:
                # å°†FlashRankè¿”å›çš„pyå­—å…¸è½¬åŒ–ä¸ºLangChainæ¥å—çš„Documentå¯¹è±¡
                doc = Document(page_content=res['text'], metadata=res['meta'])
                # print('doc:::',doc)
                doc.metadata['rerank_score'] = res['score']
                # print('doc2:::',doc)
                final_docs.append(doc)
            if len(final_docs) >= k_final:
                break

        logger.info(f"âœ… [Result] è¿”å› {len(final_docs)} ä¸ªé«˜åˆ†ç»“æœ")
        return final_docs

    # RAGæ£€ç´¢è¿”å›é€»è¾‘
    def query_formatted(self,query:str):
        """
        ç›´æ¥è¿”å›æ ¼å¼åŒ–å¥½çš„å­—ç¬¦ä¸²ï¼Œç»™Toolå’ŒWriterç”¨
        """

        results = self.query(query)

        if not results:
            return "çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚"

        # æ ¼å¼åŒ–è¿”å›ç»“æœ
        formatted_res = []
        for doc in results:
            source = doc.metadata.get('source', 'unknown')
            score = doc.metadata.get('rerank_score', 0)
            formatted_res.append(f"[æ¥æº: {source} | ç½®ä¿¡åº¦: {score:.2f}]\n{doc.page_content}")
        print('formatted_res:::', formatted_res)

        return "\n\n---\n\n".join(formatted_res)

# --- æµ‹è¯•ä»£ç  ---
if __name__ == "__main__":
    # æ¸…ç†æ—§åº“æµ‹è¯•
    if os.path.exists("./chroma_db"):
        shutil.rmtree("./chroma_db")

    rag = RAGStore()

    # æ¨¡æ‹Ÿå…¥åº“
    text = "DeepSeek-V3 æ˜¯ä¸€æ¬¾å¼ºå¤§çš„æ¨¡å‹ï¼ŒAPI ä»·æ ¼éå¸¸ä¾¿å®œã€‚SiliconFlow æä¾›äº†æé€Ÿçš„æ¨ç†æœåŠ¡ã€‚"
    rag.add_documents(text, "test_source")

    # æ¨¡æ‹Ÿæ£€ç´¢
    res = rag.query("DeepSeek æ€ä¹ˆæ ·ï¼Ÿ")
    for r in res:
        print('r:::',r)
        print('r.metadata:::',r.metadata)
        print(f"å¾—åˆ†: {r.metadata['rerank_score']:.3f} | å†…å®¹: {r.page_content}")