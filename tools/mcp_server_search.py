from mcp.server.fastmcp import FastMCP

import asyncio
from loguru import logger

from ddgs import DDGS
import trafilatura

mcp = FastMCP("SearchService",host="0.0.0.0",port=8003)


SINGLE_FETCH_TIMEOUT_SEC = 25 # å•URLè¶…æ—¶
BATCH_FETCH_TIMEOUT_SEC = 90 # æ‰¹é‡æ€»è¶…æ—¶
MAX_BATCH_CONCURRENCY = 3 # æ‰¹é‡å¹¶å‘ä¸Šé™



@mcp.tool()
async def web_search(query:str):
    """
    å¿«é€Ÿæœç´¢15ä¸ªæ‘˜è¦æ–‡ä»¶ï¼Œå†…å«æ ‡é¢˜ã€é“¾æ¥å’Œæ‘˜è¦
    """
    try:
        logger.info(f'ğŸ” [Async] æ­£åœ¨æœç´¢: {query}')
        # ç¡®ä¿æ—¶æ•ˆæ€§:æœ€è¿‘ä¸€ä¸ªæœˆ | åç»­æ›´æ–°å¯ä»¥è‡ªç”±é€‰æ‹©éœ€è¦çš„æ—¶é—´æ®µ

        # ã€æ ¸å¿ƒé€»è¾‘ã€‘ä½¿ç”¨åŒæ­¥çš„ DDGSï¼Œä½†ç”¨ to_thread åŒ…è£…æˆå¼‚æ­¥
        # ç†ç”±ï¼šDDGS å®˜æ–¹åº“å˜åŠ¨é¢‘ç¹ï¼ŒAsyncDDGS å¯èƒ½ä¸å­˜åœ¨ï¼Œè€Œ to_thread æ˜¯ Python æ ‡å‡†åº“ï¼Œæ°¸è¿œç¨³å®šã€‚
        def _sync_search():
            # max_results å»ºè®® 10-15
            # timelimit="y" (è¿‡å»ä¸€å¹´)
            return list(DDGS().text(query, max_results=15, timelimit="y"))

        # æ‰”åˆ°çº¿ç¨‹æ± è·‘ï¼Œä¸é˜»å¡ä¸»çº¿ç¨‹
        results = await asyncio.to_thread(_sync_search)

        if not results:
            return "æœªæ‰¾åˆ°ç›¸å…³ç»“æœï¼Œè¯·å°è¯•æ›´æ¢å…³é”®è¯ã€‚"

        search_results = []
        for i,r in enumerate(results):
            content = f"ç»“æœ [{i}]\næ ‡é¢˜: {r['title']}\né“¾æ¥: {r['href']}\næ‘˜è¦: {r['body']}\n"
            search_results.append(content)
        return "\n---\n".join(search_results)
    except Exception as e:
        logger.error(f"æœç´¢æœåŠ¡å‡ºé”™: {e}")
        return f'æœç´¢æœåŠ¡æš‚æ—¶ä¸å¯ç”¨: {str(e)}'


@mcp.tool()
async def get_page_content(url: str):
    """
    è·å–å•ä¸ªurlé‡Œçš„å…¨æ–‡ä¿¡æ¯
    """
    logger.info(f'âš¡ [Async] æ­£åœ¨æŠ“å–: {url}')

    def _fetch():
        downloaded = trafilatura.fetch_url(url)
        if not downloaded:
            return "Error: æ— æ³•è®¿é—®è¯¥é¡µé¢"
        result = trafilatura.extract(downloaded)
        return result or "Error: æ— æ³•æå–æ­£æ–‡å†…å®¹"

    try:
        # å•URLè¶…æ—¶å…œåº•
        return await asyncio.wait_for(
            asyncio.to_thread(_fetch),
            timeout=SINGLE_FETCH_TIMEOUT_SEC
        )
    except asyncio.TimeoutError:
        logger.warning(f"â° å•URLæŠ“å–è¶…æ—¶: {url}")
        return f"Error: æŠ“å–è¶…æ—¶ï¼ˆ>{SINGLE_FETCH_TIMEOUT_SEC}sï¼‰: {url}"
    except Exception as e:
        return f"Error: æŠ“å–æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯:{str(e)}"


@mcp.tool()
async def batch_fetch(urls: list[str]):
    """
    æ‰¹é‡è·å–urlé‡Œçš„å…¨æ–‡ä¿¡æ¯(å¹¶è¡Œ)
    å¦‚æœæ˜¯æ‰¹é‡è·å–ï¼Œä¼˜å…ˆä½¿ç”¨è¯¥å·¥å…·
    """
    logger.info(f'æ­£åœ¨æ‰¹é‡è·å–{len(urls)}ä¸ªURLçš„å…¨æ–‡ä¿¡æ¯...')
    sem = asyncio.Semaphore(MAX_BATCH_CONCURRENCY)
    async def fetch_one(url:str):
        async with sem:
            try:
                return await asyncio.wait_for(
                    get_page_content(url),
                    timeout=SINGLE_FETCH_TIMEOUT_SEC
                )
            except Exception as e:
                return f"Error: {url} æŠ“å–å¤±è´¥: {e}"
    try:
        results = await asyncio.wait_for(
            asyncio.gather(*(fetch_one(url) for url in urls)),
            timeout=BATCH_FETCH_TIMEOUT_SEC
        )
    except asyncio.TimeoutError:
        return "Error: æ‰¹é‡æŠ“å–è¶…æ—¶ï¼Œè¯·å‡å°‘URLæ•°é‡åé‡è¯•ã€‚"
    return "\n\n=== æ–‡ç« åˆ†éš”çº¿ ===\n\n".join(results)

if __name__ == '__main__':
    mcp.run("streamable-http")