import httpx
import requests
from mcp.server.fastmcp import FastMCP

import asyncio
from loguru import logger

from ddgs import DDGS


mcp = FastMCP("SearchService",host="0.0.0.0",port=8003)


# headers = {
#     "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
# }

@mcp.tool()
async def web_search(query:str):
    """
    å¿«é€Ÿæœç´¢15ä¸ªæ‘˜è¦æ–‡ä»¶ï¼Œå†…å«æ ‡é¢˜ã€é“¾æ¥å’Œæ‘˜è¦
    """
    try:
        logger.info(f'ğŸ” [Async] æ­£åœ¨æœç´¢: {query}')
        # ç¡®ä¿æ—¶æ•ˆæ€§:æœ€è¿‘ä¸€ä¸ªæœˆ | åç»­æ›´æ–°å¯ä»¥è‡ªç”±é€‰æ‹©éœ€è¦çš„æ—¶é—´æ®µ

        # ã€æ ¸å¿ƒé€»è¾‘ã€‘ä½¿ç”¨åŒæ­¥çš„ DDGSï¼Œä½†ç”¨ to_thread åŒ…è£…æˆå¼‚æ­¥
        # ç†ç”±ï¼šDDGS å®˜æ–¹åº“å˜åŠ¨é¢‘ç¹ï¼ŒAsyncDDGS å¯èƒ½ä¸å­˜åœ¨ï¼Œè€Œ to_thread æ˜¯ Python æ ‡å‡†åº“ï¼Œæ°¸è¿œç¨³å®šã€‚
        def _sync_search():
            # max_results å»ºè®® 10-15
            # timelimit="y" (è¿‡å»ä¸€å¹´)
            return list(DDGS().text(query, max_results=15, timelimit="y"))

        # æ‰”åˆ°çº¿ç¨‹æ± è·‘ï¼Œä¸é˜»å¡ä¸»çº¿ç¨‹
        results = await asyncio.to_thread(_sync_search)

        if not results:
            return "æœªæ‰¾åˆ°ç›¸å…³ç»“æœï¼Œè¯·å°è¯•æ›´æ¢å…³é”®è¯ã€‚"

        search_results = []
        for i,r in enumerate(results):
            content = f"ç»“æœ [{i}]\næ ‡é¢˜: {r['title']}\né“¾æ¥: {r['href']}\næ‘˜è¦: {r['body']}\n"
            search_results.append(content)
        return "\n---\n".join(search_results)
    except Exception as e:
        logger.error(f"æœç´¢æœåŠ¡å‡ºé”™: {e}")
        return f'æœç´¢æœåŠ¡æš‚æ—¶ä¸å¯ç”¨: {str(e)}'

@mcp.tool()
async def get_page_content(url:str):
    """
    è·å–å•ä¸ªurlé‡Œçš„å…¨æ–‡ä¿¡æ¯
    """
    logger.info(f'âš¡ [Async] æ­£åœ¨æŠ“å–: {url}')
    real_url = f"https://r.jina.ai/{url}"

    async with httpx.AsyncClient(timeout=30.0,follow_redirects=True) as client: # è‡ªåŠ¨è·Ÿéšé‡å®šå‘
        try:
            headers = {"User-Agent": "Mozilla/5.0"}
            response = await client.get(real_url,headers=headers)
            result = response.text
            return result
        except httpx.TimeoutException as e:
            return "Error:è¯·æ±‚è¶…æ—¶ï¼Œç½‘é¡µå“åº”å¤ªæ…¢"
        except Exception as e:
            return f"Error:æŠ“å–æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯:{str(e)}"

@mcp.tool()
async def batch_fetch(urls:list[str]):
    """
    æ‰¹é‡è·å–urlé‡Œçš„å…¨æ–‡ä¿¡æ¯(å¹¶è¡Œ)
    å¦‚æœæ˜¯æ‰¹é‡è·å–ï¼Œä¼˜å…ˆä½¿ç”¨è¯¥å·¥å…·
    """
    print(f'æ­£åœ¨æ‰¹é‡è·å–{len(urls)}ä¸ªURLçš„å…¨æ–‡ä¿¡æ¯...')
    tasks = [get_page_content(url) for url in urls]
    contents = await asyncio.gather(*tasks)
    # è¿™é‡Œè¿”å›å¿…é¡»æ˜¯strï¼Œä¸ç„¶å·¥å…·è¿”å›æ¥æ”¶å¯èƒ½å› ä¸ºçœ‹åˆ°çš„ä¸æ˜¯strè€ŒæŠ¥é”™
    return "\n\n=== æ–‡ç« åˆ†éš”çº¿ ===\n\n".join(contents)



if __name__ == '__main__':
    mcp.run("streamable-http")
    # mcp.run()